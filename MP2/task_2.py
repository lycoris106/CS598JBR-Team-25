import jsonlines
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import os

#####################################################
# Please finish all TODOs in this file for MP2;
#####################################################

def save_file(content, file_path):
    with open(file_path, 'w') as file:
        file.write(content)

# After the model generates a Python test suite (if the model does not generate a test class with multiple tests, you can parse the response and create it yourself), you should run the tests to check if they pass or fail. For a given problem ID, assuming that your code is saved with name {ID}.py and you save the modelâ€™s response into a file {ID}_test.py (this file should be separate from the main code), use the command `pytest {ID}_test.py` to execute the tests generated by the model. You should also generate the test coverage report. To get the coverage of the generated tests, you can use the command `pytest {ID}_test.py --cov {ID} --cov-report json:MP2/Coverage/{ID}_test_{vanilla/crafted}.json`. This will save the coverage report ({ID}_test_{vanilla/crafted}.json) under your repo's MP2/Coverage path. When using the initial prompt, test coverage may not be that high, and you should improve the prompt to generate higher coverage test suites (please carefully read the note at the beginning of MP about the importance of improving LLM performance through prompt engineering and losing points).
def generate_coverage(task_id, program_str, response):
    # create a folder for codes if not exists
    if not os.path.exists("Codes"):
        os.makedirs("Codes")
    suffix = "vanilla" if vanilla else "crafted"
    response = "import * from Codes.{task_id}\n" + response
    save_file(program_str, f"Codes/{task_id}.py")
    save_file(response, f"Codes/{task_id}_test.py")
    os.system(f"pytest Codes/{task_id}_test.py --cov {task_id} --cov-report json:Coverage/{task_id}_test_{suffix}.json")
    return f"Coverage/{task_id}_test_{suffix}.json"

def prompt_model(dataset, model_name = "deepseek-ai/deepseek-coder-6.7b-instruct", vanilla = True):
    print(f"Working with {model_name} prompt type {vanilla}...")
    
    # TODO: download the model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # TODO: load the model with quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,     
        device_map="auto",
        torch_dtype=torch.bfloat16,
        quantization_config=bnb_config
    )
    
    results = []
    for entry in dataset:
        # TODO: create prompt for the model
        # Tip : Use can use any data from the dataset to create 
        #       the prompt including prompt, canonical_solution, test, etc.
        # TASK_PROMPT = entry['prompt']
        program_str = entry['canonical_solution']


        prompt = f"""You are an AI programming assistant. You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.
### Instruction:
Generate a pytest test suite for the following code.
Only write unit tests in the output and nothing else.
{program_str}
"""
        # TODO: prompt the model and get the response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_length=500,
                do_sample=False,
                # temperature=0.0, No need when do_sample=False
            )
        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        # TODO: process the response, generate coverage and save it to results
        coverage = generate_coverage(entry['task_id'], program_str, response)

        print(f"Task_ID {entry['task_id']}:\nprompt:\n{prompt}\nresponse:\n{response}\ncoverage:\n{coverage}")
        results.append({
            "task_id": entry["task_id"],
            "prompt": prompt,
            "response": response,
            "coverage": coverage
        })
        
    return results

def read_jsonl(file_path):
    dataset = []
    with jsonlines.open(file_path) as reader:
        for line in reader: 
            dataset.append(line)
    return dataset

def write_jsonl(results, file_path):
    with jsonlines.open(file_path, "w") as f:
        for item in results:
            f.write_all([item])

if __name__ == "__main__":
    """
    This Python script is to run prompt LLMs for code synthesis.
    Usage:
    `python3 task_2.py <input_dataset> <model> <output_file> <if_vanilla>`|& tee prompt.log

    Inputs:
    - <input_dataset>: A `.jsonl` file, which should be your team's dataset containing 20 HumanEval problems.
    - <model>: Specify the model to use. Options are "deepseek-ai/deepseek-coder-6.7b-base" or "deepseek-ai/deepseek-coder-6.7b-instruct".
    - <output_file>: A `.jsonl` file where the results will be saved.
    - <if_vanilla>: Set to 'True' or 'False' to enable vanilla prompt
    
    Outputs:
    - You can check <output_file> for detailed information.
    """
    args = sys.argv[1:]
    input_dataset = args[0]
    model = args[1]
    output_file = args[2]
    if_vanilla = args[3] # True or False
    
    if not input_dataset.endswith(".jsonl"):
        raise ValueError(f"{input_dataset} should be a `.jsonl` file!")
    
    if not output_file.endswith(".jsonl"):
        raise ValueError(f"{output_file} should be a `.jsonl` file!")
    
    vanilla = True if if_vanilla == "True" else False
    
    dataset = read_jsonl(input_dataset)
    results = prompt_model(dataset, model, vanilla)
    write_jsonl(results, output_file)
